Someone just served Llama 3.1 at seventeen thousand tokens per second using custom silicon. That's not a typo — seventeen thousand.

Canadian startup Taalas just dropped their custom ASIC chip that makes current AI inference look glacially slow. Meanwhile OpenAI's quietly flexing too — their new GPT-5.3-Codex-Spark is now cranking out twelve hundred tokens per second, thirty percent faster than before. The speed wars are heating up and honestly, this changes everything for real-time AI.

Google fired back at OpenAI today with Gemini 3.1 Pro, claiming it scores twice as high as the previous version on ARC-AGI tests. Translation: Google thinks they're winning the reasoning race now, and they want everyone to know it.

OpenAI showed us their AI's first attempts at proving graduate-level math theorems for something called the First Proof challenge. The submissions are messy, the logic is wonky, but watching an AI try to think through expert math problems? Kind of wild to see the thought process laid bare.

Hugging Face just absorbed GGML and llama.cpp — that's the code powering most local AI setups right now. Big move for keeping AI models running on your laptop instead of big tech's servers, and honestly not sure if this centralizes things more or keeps them open.

Full details and links in the description — catch you tomorrow.