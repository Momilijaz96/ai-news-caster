Hey, it's February 21st — here's what's happening in AI today.

Three things today — first, GGML and llama.cpp are officially joining Hugging Face to push local AI forward. Second, Google just dropped Gemini 3.1 Pro with 2x better performance on reasoning benchmarks at half the price of Claude. Third, a Canadian startup called Taalas built custom silicon that runs Llama 3.1 8B at a ridiculous 17,000 tokens per second. Let's get into it.

Alright, starting with this huge news from Hugging Face. GGML and llama.cpp are officially joining the company. Now, if you're doing any local AI work, you've definitely used these tools. GGML is the machine learning library that makes it possible to run large language models on consumer hardware, and llama.cpp is probably the most popular way to actually run models locally on your laptop or desktop.

This is actually a really smart move by Hugging Face. They're basically cementing themselves as the hub for both cloud and local AI development. Think about it — they already host the models, they provide the training infrastructure, and now they're bringing in the team that built the most widely used local inference tools. It's like they're covering every possible way you might want to work with AI models.

For developers, this is great news. It means GGML and llama.cpp are going to get more resources and probably better integration with the broader Hugging Face ecosystem. I'm expecting we'll see tighter integration between local model formats and Hugging Face's model hub, maybe better tooling for converting and optimizing models for local use. The fact that Hugging Face is investing in local AI infrastructure tells me they see the writing on the wall — not everything needs to run in the cloud, and there's real demand for running models locally for privacy, cost, or latency reasons.

Next up, Google's new Gemini 3.1 Pro is making waves. This thing is scoring 2x better than Gemini 3.0 on ARC-AGI benchmarks, which test reasoning and problem-solving abilities. But here's the kicker — it's priced at $2 per million input tokens and $12 per million output tokens for contexts under 200K tokens. That's less than half the price of Claude Opus 4.6 while delivering very similar performance.

This is getting interesting because we're seeing real price competition at the high end. Google is essentially saying "we can match the reasoning capabilities of the most expensive models at half the cost." For developers building production applications, this changes the economics significantly. You could potentially afford to use more sophisticated reasoning in your apps because the cost barrier just got a lot lower.

GitHub is already rolling this out in Copilot as a public preview, and they're specifically highlighting its performance on edit-then-test loops. That's a good sign — it means Google optimized this model for the kind of iterative development work that developers actually do. I'm curious to see how it performs in real-world coding tasks compared to the current crop of coding-focused models.

Now here's where things get really wild. This Canadian startup Taalas just announced their HC1 chip that can run Llama 3.1 8B at 16,960 tokens per second. That's not a typo — nearly 17,000 tokens per second. To put that in perspective, that's fast enough that it would look like text teleporting onto your screen rather than streaming.

This is part of what they're calling the "custom ASIC thesis" — the idea that we're moving beyond general-purpose GPUs toward specialized chips designed specifically for AI inference. Taalas built silicon optimized for transformer architectures, and the results speak for themselves. We're talking about inference speeds that make real-time conversation with AI feel instantaneous.

What's really interesting is the economics here. These custom chips are expensive to develop, but if you can achieve 10x or 20x speedups over traditional hardware, the cost per token could actually be lower while providing a dramatically better user experience. For developers, this opens up entirely new categories of applications that were impossible before due to latency constraints.

I think we're going to see more of this. The big cloud providers are already building their own AI chips, but we're starting to see specialized startups tackle specific use cases. Taalas is focusing on inference, but I wouldn't be surprised to see similar approaches for training, for edge deployment, for specific model architectures.

Switching gears to OpenAI, they just shared their submissions to the First Proof math challenge. This is OpenAI testing their models on research-grade mathematical reasoning — the kind of problems that would challenge graduate students and researchers. They're being transparent about both successes and failures, which I appreciate.

This matters because mathematical reasoning is one of the hardest benchmarks for AI systems. It requires not just pattern matching but genuine logical reasoning, the ability to construct valid proofs step by step. If AI systems can start making meaningful contributions to mathematical research, that's a strong signal about their reasoning capabilities more broadly.

The fact that OpenAI is sharing their attempts publicly, including the failures, suggests they're confident enough in their progress to be transparent. It's also good for the research community — other teams can learn from these approaches and build on them.

On the development tools front, there's some solid news from GitHub. They're rolling out organization-level Copilot usage metrics dashboards. Previously, these were only available at the enterprise level, but now smaller organizations can get visibility into how their teams are using Copilot.

This is actually pretty important for teams trying to justify their Copilot subscriptions or understand their ROI. The dashboard shows things like pull request throughput and time to merge for Copilot users versus non-users. Having that data at the organization level makes it much easier for smaller companies to make data-driven decisions about their AI tooling investments.

They're also adding more model options for Copilot's coding agent. Business and Enterprise users can now choose between different models depending on their specific needs. This is part of a broader trend toward giving developers more control over which AI models power their tools.

Speaking of models, GitHub is deprecating some older Anthropic and OpenAI models from Copilot. This is normal housekeeping, but it's worth noting if you've been relying on specific model behaviors. They're pushing users toward newer, more capable models.

There's also better integration between GitHub's various tools. You can now see all pull request comments directly in the Files changed page without jumping back and forth between tabs. It sounds minor, but these workflow improvements add up when you're doing code reviews all day.

One more quick development story — GitHub's workflow dispatch API now returns run IDs when you trigger workflows programmatically. This makes it much easier to track the workflows you've triggered and build automation around GitHub Actions. It's the kind of API improvement that makes developers' lives easier without making headlines.

Hugging Face is also making some moves in the training space. They're offering free AI model training through a partnership with Unsloth and their Jobs platform. This could democratize access to model training for individual developers and smaller organizations who can't afford expensive GPU clusters.

Lastly, there's interesting discussion happening around AI coding tools and their impact on development workflows. Simon Willison shared a story about losing code while working with multiple AI agents and development environments, then recovering it from Claude's session logs. It's a glimpse into how development workflows are evolving when you're collaborating with AI systems that maintain their own project state.

This kind of workflow is becoming more common — developers working across multiple AI-powered environments, with code and context scattered across different systems. We're going to need better tooling to manage this complexity as AI agents become more capable and autonomous.

The broader theme I'm seeing across all these stories is maturation. The AI tooling ecosystem is getting more sophisticated, more integrated, and more focused on real developer needs. We're past the "wow, AI can write code" phase and into the "how do we build robust development workflows around AI" phase.

That's it for today — catch you tomorrow.