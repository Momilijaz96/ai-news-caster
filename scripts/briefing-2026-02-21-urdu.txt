السلام علیکم دوست! آج 21 فروری ہے اور واہ، آج AI کی دنیا میں کچھ زبردست چیزیں ہوئی ہیں جن کے بارے میں بتانا ضروری تھا!

آج تین چیزیں جو مجھے بہت پسند آئیں — پہلی: Taalas نام کی Canadian company نے Llama 3.1 8B کو سترہ ہزار tokens فی سیکنڈ کی رفتار سے چلایا اپنے custom hardware پر، یہ تو کمال ہے! دوسری: Google کا نیا Gemini 3.1 Pro آ گیا جو ARC-AGI پر بہت اچھی performance دے رہا ہے اور GitHub Copilot میں بھی available ہے۔ تیسری: OpenAI نے GPT-5.3-Codex-Spark کو 30 فیصد تیز کر دیا اور اب یہ بارہ سو tokens فی سیکنڈ serve کر رہا ہے۔ چلو تفصیل میں چلتے ہیں!

اچھا تو سب سے پہلی اور سب سے exciting چیز کرتے ہیں۔ Taalas کا یہ announcement واقعی mind blowing ہے یار! یہ Canadian startup ہے جو custom ASICs بنا رہی ہے AI models کے لیے۔ انہوں نے اپنا پہلا product announce کیا ہے جو Llama 3.1 8B کو dedicated hardware پر run کرتا ہے۔ اب سنو اس کی speed — سترہ ہزار tokens فی سیکنڈ! یہ اتنا تیز ہے کہ اگر آپ کو video دکھایا جائے تو آپ کو لگے گا کہ یہ fake ہے۔ معلوم کیوں؟ کیونکہ text اتنی تیزی سے generate ہوتا ہے کہ human eye میں بس blur نظر آئے گا۔

یہ کیوں اہم ہے؟ دیکھو یار، اب تک ہم سب GPU vendors کے رحم و کرم پر تھے، خاص طور پر NVIDIA کے۔ لیکن یہ custom ASIC approach بہت game changing ہے کیونکہ یہ specific model کے لیے optimized ہے۔ جب آپ general purpose GPU استعمال کرتے ہیں تو بہت ساری processing power waste ہوتی ہے ان کاموں پر جن کی ضرورت نہیں۔ لیکن custom silicon میں ہر transistor اس specific model کے لیے designed ہوتا ہے۔

البتہ یہاں ایک بڑا trade-off ہے۔ یہ hardware صرف Llama 3.1 8B run کر سکتا ہے، کوئی اور model نہیں۔ تو flexibility کی قربانی دے کر آپ کو extreme performance مل رہی ہے۔ میں سمجھتی ہوں کہ یہ approach ان companies کے لیے بہت اچھا ہے جو production میں ایک specific model scale پر run کرنا چاہتی ہیں۔

اب دوسری بات کرتے ہیں — Google کا Gemini 3.1 Pro! یہ تو واقعی solid launch لگ رہا ہے۔ پہلے ARC-AGI benchmarks کی بات کرتے ہیں۔ ARC-AGI یہ reasoning benchmark ہے جو AI models کی abstract reasoning capabilities test کرتا ہے۔ Gemini 3.1 Pro نے اس میں اپنے پچھلے version سے دوگنا بہتر performance دکھایا ہے۔

اب دیکھو pricing کی بات — یہ Claude Opus 4.6 سے آدھی قیمت میں available ہے لیکن benchmarks میں very similar performance دے رہا ہے۔ یہ developers کے لیے بہت اچھی خبر ہے کیونکہ competition سے prices down آتی ہیں۔ دو ڈالر فی million input tokens اور بارہ ڈالر فی million output tokens اگر آپ دو لاکھ tokens سے کم استعمال کر رہے ہیں۔

GitHub نے بھی فوری طور پر اسے Copilot میں integrate کر دیا ہے public preview میں۔ GitHub کا کہنا ہے کہ یہ model خاص طور پر agentic coding میں اچھی performance دے رہا ہے اور edit-then-test loops میں بہت effective ہے۔ میں نے ابھی تک personally test نہیں کیا لیکن early reports کافی promising ہیں۔

تیسری بڑی خبر OpenAI سے ہے۔ انہوں نے GPT-5.3-Codex-Spark کو optimize کیا ہے اور اب یہ بارہ سو tokens فی سیکنڈ serve کر رہا ہے۔ یہ 30 فیصد improvement ہے پچھلی performance سے۔ Thibault Sottiaux نے یہ announce کیا Twitter پر۔

اب یہ speed تو Taalas کی custom hardware جتنی نہیں، لیکن یہ general purpose ہے اور GPT-5 series کا model ہے جو capabilities میں کافی advanced ہوگا۔ یہ balance اہم ہے — آپ کو flexibility بھی چاہیے اور performance بھی۔

ایک اور interesting بات ہوئی ہے — HuggingFace نے announce کیا ہے کہ GGML اور llama.cpp teams ان کے ساتھ join کر رہے ہیں local AI development کو آگے بڑھانے کے لیے۔ یہ بہت بڑا move ہے open source AI community کے لیے۔ GGML اور llama.cpp یہ local inference کے لیے بہت مشہور tools ہیں۔ جب یہ HuggingFace ecosystem کے ساتھ properly integrate ہوں گے تو local AI development اور بھی آسان ہو جائے گی۔

OpenAI نے ایک اور interesting thing share کی ہے — انہوں نے First Proof math challenge کے لیے اپنے AI model کی proof attempts share کیں ہیں۔ یہ research-grade reasoning کا test ہے expert-level mathematical problems پر۔ یہ academic community کے لیے بہت valuable insight ہے کہ current AI models mathematical reasoning میں کہاں کھڑے ہیں۔

Simon Willison کے blog پر ایک دلچسپ بات پڑھی — Andrej Karpathy نے "Claws" کے بارے میں بات کی ہے۔ انہوں نے Mac Mini خریدا ہے اس کے ساتھ experiment کرنے کے لیے۔ Apple store والے نے بتایا کہ Mac Minis اجکل hotcakes کی طرح بک رہے ہیں اور سب confused ہیں کہ اتنی demand کیوں ہے۔ شاید یہ local AI development کی وجہ سے ہے۔

GitHub کی طرف سے بھی کئی updates آئے ہیں۔ انہوں نے organization-level Copilot usage metrics dashboard launch کیا ہے public preview میں۔ پہلے یہ صرف enterprise level پر available تھا۔ اب smaller organizations بھی track کر سکتے ہیں کہ ان کے developers Copilot کتنا اور کیسے استعمال کر رہے ہیں۔

ایک اور GitHub update یہ ہے کہ Zed editor میں اب officially GitHub Copilot support ہے۔ Zed یہ نیا code editor ہے جو performance کے لیے famous ہے۔ GitHub نے formal partnership کی ہے تو اب آپ اپنی Copilot Pro یا Business subscription Zed میں بھی استعمال کر سکتے ہیں۔

اب ایک technical چیز بھی share کرنا چاہوں گی۔ Thariq Shihipar نے prompt caching کے بارے میں tweet کیا ہے کہ یہ کتنا important ہے long running agentic products کے لیے۔ جیسے Claude Code میں وہ اپنا پورا system prompt caching کے around build کرتے ہیں۔ جب cache hit rate high ہوتی ہے تو costs کم آتی ہیں اور latency بھی کم ہوتی ہے۔ یہ ان لوگوں کے لیے useful information ہے جو production میں AI agents build کر رہے ہیں۔

بس یار، یہ تھا آج کا! Hardware سے لے کر models تک، optimization سے لے کر tooling تک — بہت کچھ ہو رہا ہے AI space میں۔ سب سے exciting میں لگ رہی ہے custom hardware والی developments کیونکہ یہ cost اور performance دونوں میں game changer ثابت ہو سکتی ہیں۔ کل پھر ملتے ہیں نئی خبروں کے ساتھ!